# -*- coding: utf-8 -*-
"""RAG Final Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iaUW8DpfVVI3iFo20z70sndZqA2v773E
"""

! pip install  transformers faiss-cpu sentence-transformers

import json
import os

# List of file names you want to combine
file_names = ['MVA.json', 'cpc.json', 'crpc_new.json','ida.json','iea_new.json','ipc_new.json','nia_new.json']

# Initialize an empty list to hold all the data
combined_data = []

# Loop over the list of files and read in the data
for file_name in file_names:
    with open(file_name, 'r', encoding='utf-8') as file:
        data = json.load(file)
        # Remove the 'section' key from each entry
        for entry in data:
            if 'section' in entry:
                del entry['section']
        # Assuming each file contains a list of entries
        combined_data.extend(data)
# Encode the data for RAG use
with open('combined.json', 'w', encoding='utf-8') as file:
    json.dump(combined_data, file, ensure_ascii=False, indent=4)

import json

# Assuming 'combined.json' is the file with combined entries without 'section' key
combined_file_name = 'combined.json'
jsonl_file_name = 'combined.jsonl'

# Read the combined JSON file
with open(combined_file_name, 'r', encoding='utf-8') as json_file:
    combined_data = json.load(json_file)

# Write each entry as a single line in the JSONL file
with open(jsonl_file_name, 'w', encoding='utf-8') as jsonl_file:
    for entry in combined_data:
        jsonl_file.write(json.dumps(entry, ensure_ascii=False) + '\n')

from sentence_transformers import SentenceTransformer, util
import json
import numpy as np
import faiss

# Specify the model you wish to use
model_name = 'sentence-transformers/all-mpnet-base-v2'  # This model requires less memory and is faster

# Load the .jsonl file and extract the text
def load_jsonl(file_path):
    texts = []
    with open(file_path, 'r') as jsonl_file:
        for line in jsonl_file:
            entry = json.loads(line)
            text = entry.get("description", "").strip()
            if text:
                texts.append(text)
    return texts

# Generate embeddings for a list of texts
def get_embeddings(texts):
    model = SentenceTransformer(model_name)
    return model.encode(texts, show_progress_bar=True)

# Load texts from the .jsonl file
jsonl_file_path = 'combined.jsonl'
texts = load_jsonl(jsonl_file_path)

# Generate embeddings
embeddings = get_embeddings(texts)

# FAISS index creation
embedding_dim = embeddings.shape[1]
index = faiss.IndexFlatL2(embedding_dim)
index.add(np.array(embeddings).astype('float32'))

# Function to retrieve documents using FAISS index
def retrieve_documents(query, k=5):
    query_embedding = get_embeddings([query])[0]
    distances, indices = index.search(np.array([query_embedding]).astype('float32'), k)
    return [texts[i] for i in indices[0]]

# # Example usage

# print(retrieved_docs)

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
llm_model_name = 'allenai/unifiedqa-v2-t5-large-1363200'  # Example model, you can use any model you prefer
tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
llm_model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)

# Generate a response with the LLM

# Generate a response with the LLM with adjusted parameters
def generate_response(input_text):
    inputs = tokenizer.encode("context: " + input_text, return_tensors="pt", add_special_tokens=True)

    # Adjust generation parameters
    outputs = llm_model.generate(
        inputs,
        max_length=300,  # Increase max_length for potentially longer output
        min_length=150,   # Set a minimum length to ensure a longer response
        temperature=0.9,  # Adjust temperature if you want more creative responses
        num_beams=10,     # Increase the number of beams for more thorough search
        no_repeat_ngram_size=2,  # Prevent repeating the same information
        early_stopping=True
    )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Retrieve documents related to the query

query = 'What is fraud?'
retrieved_docs = retrieve_documents(query,k=3)

# Construct the prompt
prompt = "You have detailed knowledge of Indian Law. Here are a list of related documents:" + "\n".join(retrieved_docs) + "\nPlease write the most relevant answer to the next question" + query

# Generate the response using the LLM
response = generate_response(prompt)

print(response)